# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tg0nuNMQp4S9VhQhqHvr1NWgmX3xn0G_
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Load and preprocess
df = pd.read_csv("final_data.csv")
df['time'] = pd.to_datetime(df['time'])
df = df.sort_values('time')
df = df.drop(columns=['wave_sin_dir', 'wave_cos_dir'])

# Add time features
df['month'] = df['time'].dt.month
df['dayofweek'] = df['time'].dt.dayofweek

features = ['beach.x', 'crt_salt', 'crt_temp', 'crt_u', 'crt_v',
            'wnd_sfcWindspeed', 'wnd_uas', 'wnd_vas', 'wave_hs',
            'wave_t01', 'wave_fp', 'wave_dir', 'wnd_dir', 'month', 'dayofweek']
target = 'presence'

# Normalize features
scaler = MinMaxScaler()
df_scaled = df.copy()
df_scaled[features] = scaler.fit_transform(df[features])

# Create sequences
SEQ_LEN = 14
def create_sequences(data, labels, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(labels[i+seq_len])
    return np.array(X), np.array(y)

X_all, y_all = create_sequences(df_scaled[features].values, df[target].values, SEQ_LEN)

# Train/test split
split = int(len(X_all) * 0.8)
X_train, X_test = X_all[:split], X_all[split:]
y_train, y_test = y_all[:split], y_all[split:]

# Class weights for imbalance
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))

# Model functions
def build_vanilla(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(50, activation='tanh'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_stacked(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(50, activation='tanh', return_sequences=True),
        LSTM(30, activation='tanh'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_bidirectional(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Bidirectional(LSTM(50, activation='tanh')),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_dropout(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(50, activation='tanh'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Model
model_dict = {
    "Vanilla LSTM": build_vanilla, #1x LSTM, Simple temporal dependencies
    "Stacked LSTM": build_stacked, #2x LSTM, Hierarchical patterns
    "Bidirectional LSTM": build_bidirectional, # 1x Bi-LSTM, Bidirectional pattern recognition
    "Dropout LSTM": build_dropout # 1x LSTM + Dropout, Preventing overfitting
}

# Train, evaluate, collect results
results = []
histories = {}
for name, build_fn in model_dict.items():
    print(f"\nTraining {name}...")
    model = build_fn((SEQ_LEN, len(features)))
    early_stop = EarlyStopping(patience=10, restore_best_weights=True)
    history = model.fit(X_train, y_train, validation_split=0.1, epochs=50, batch_size=16,
                        class_weight=class_weights_dict, callbacks=[early_stop], verbose=0)
    histories[name] = history
    y_pred_prob = model.predict(X_test).flatten()
    y_pred = (y_pred_prob > 0.5).astype(int)
    print(f"--- {name} Evaluation ---")
    print(classification_report(y_test, y_pred, digits=3))
    print(confusion_matrix(y_test, y_pred))

"""Bidirectional LSTM good in :

- Recall (better at detecting actual bluebottle presence)

- F1-score (balanced precision/recall for minority class)    


Vanilla LSTM has high precision for class 0 but fails on class 1 (presence).

Class Imbalance not good:

- Only 46 out of 665 test examples are positive (presence = 1)

- Most models are biased toward predicting 0

# beach survey --- train; randwick--- test
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Input
from tensorflow.keras.callbacks import EarlyStopping


df = pd.read_csv("final_data.csv")
df['time'] = pd.to_datetime(df['time'])
df = df.sort_values("time").reset_index(drop=True)

# Add time-based features
df['month'] = df['time'].dt.month
df['dayofweek'] = df['time'].dt.dayofweek

# Create 'presence' if not already present
if 'presence' not in df.columns and 'bluebottles' in df.columns:
    df['presence'] = df['bluebottles'].fillna(0).apply(lambda x: 1 if x > 0 else 0)

# Compute wind direction if missing
if 'wnd_dir' not in df.columns and 'wnd_uas' in df.columns and 'wnd_vas' in df.columns:
    df['wnd_dir'] = (np.degrees(np.arctan2(df['wnd_uas'], df['wnd_vas'])) + 360) % 360

# Define multiple feature combinations to try
feature_sets = [
    ['crt_u', 'wave_hs', 'wnd_dir', 'month', 'dayofweek'],
    ['crt_temp', 'crt_salt', 'wave_dir', 'wnd_dir', 'month', 'dayofweek'],
    ['crt_temp', 'wave_dir', 'wave_hs', 'wave_fp', 'month', 'dayofweek'],
    ['crt_temp', 'wave_dir', 'wnd_dir', 'wave_t01', 'wave_fp', 'month', 'dayofweek'],
    ['crt_temp', 'crt_u', 'crt_v', 'wave_dir', 'wave_hs', 'wave_t01', 'wave_fp', 'wnd_dir', 'month', 'dayofweek']
]

# LSTM
def run_lstm_experiment(df, feature_list, seq_len=14):
    print(f"\n Running experiment with features:\n{feature_list}\n")

    # Time-based split
    split_index = int(len(df) * 0.8)
    df_train = df.iloc[:split_index].copy()
    df_test = df.iloc[split_index:].copy()

    # Scale features
    scaler = MinMaxScaler()
    df_train_scaled = df_train.copy()
    df_test_scaled = df_test.copy()
    df_train_scaled[feature_list] = scaler.fit_transform(df_train[feature_list])
    df_test_scaled[feature_list] = scaler.transform(df_test[feature_list])

    # Create sequences
    def create_sequences(data, labels, seq_len):
        X, y = [], []
        for i in range(len(data) - seq_len):
            X.append(data[i:i+seq_len])
            y.append(labels[i+seq_len])
        return np.array(X), np.array(y)

    X_train, y_train = create_sequences(df_train_scaled[feature_list].values, df_train_scaled['presence'].values, seq_len)
    X_test, y_test = create_sequences(df_test_scaled[feature_list].values, df_test_scaled['presence'].values, seq_len)

    # Class weights
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weights_dict = dict(enumerate(class_weights))

    # Build Bidirectional LSTM
    model = Sequential([
        Input(shape=(seq_len, len(feature_list))),
        Bidirectional(LSTM(50, activation='tanh')),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    early_stop = EarlyStopping(patience=10, restore_best_weights=True)

    history = model.fit(X_train, y_train, validation_split=0.1, epochs=50, batch_size=16,
                        class_weight=class_weights_dict, callbacks=[early_stop], verbose=0)

    # Predict and evaluate
    y_pred_prob = model.predict(X_test).flatten()

    for threshold in [0.5, 0.6, 0.7, 0.8]:
        print(f"\n Threshold = {threshold}")
        y_pred = (y_pred_prob > threshold).astype(int)
        print(classification_report(y_test, y_pred, digits=3))
        print("Confusion Matrix:")
        print(confusion_matrix(y_test, y_pred))

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.title("ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.grid(True)
    plt.show()

    # PR Curve
    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)
    plt.figure(figsize=(6, 4))
    plt.plot(recall, precision)
    plt.title("Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.grid(True)
    plt.show()

# Loop over feature sets
for idx, feature_list in enumerate(feature_sets):
    print(f"\n=======================\n Trial {idx + 1} of {len(feature_sets)}\n=======================\n")
    run_lstm_experiment(df, feature_list)

"""Best combination:
['crt_temp', 'wave_dir', 'wave_hs', 'wave_t01', 'wave_fp', 'wnd_dir', 'month', 'dayofweek']

- At Threshold = 0.8:

- Accuracy: 76.8%

- Recall (presence): 28.2%

- Precision (presence): 8.8%

- F1 (presence): 0.134

- True Positives: 69 / 245 presence events
"""

