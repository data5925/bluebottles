# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tg0nuNMQp4S9VhQhqHvr1NWgmX3xn0G_
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Load and preprocess
df = pd.read_csv("final_data.csv")
df['time'] = pd.to_datetime(df['time'])
df = df.sort_values('time')
df = df.drop(columns=['wave_sin_dir', 'wave_cos_dir'])

# Add time features
df['month'] = df['time'].dt.month
df['dayofweek'] = df['time'].dt.dayofweek

features = ['beach.x', 'crt_salt', 'crt_temp', 'crt_u', 'crt_v',
            'wnd_sfcWindspeed', 'wnd_uas', 'wnd_vas', 'wave_hs',
            'wave_t01', 'wave_fp', 'wave_dir', 'wnd_dir', 'month', 'dayofweek']
target = 'presence'

# Normalize features
scaler = MinMaxScaler()
df_scaled = df.copy()
df_scaled[features] = scaler.fit_transform(df[features])

# Create sequences
SEQ_LEN = 14
def create_sequences(data, labels, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(labels[i+seq_len])
    return np.array(X), np.array(y)

X_all, y_all = create_sequences(df_scaled[features].values, df[target].values, SEQ_LEN)

# Train/test split
split = int(len(X_all) * 0.8)
X_train, X_test = X_all[:split], X_all[split:]
y_train, y_test = y_all[:split], y_all[split:]

# Class weights for imbalance
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))

# Model functions
def build_vanilla(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(50, activation='tanh'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_stacked(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(50, activation='tanh', return_sequences=True),
        LSTM(30, activation='tanh'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_bidirectional(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Bidirectional(LSTM(50, activation='tanh')),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_dropout(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(50, activation='tanh'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Model
model_dict = {
    "Vanilla LSTM": build_vanilla,
    "Stacked LSTM": build_stacked,
    "Bidirectional LSTM": build_bidirectional,
    "Dropout LSTM": build_dropout
}

# Train, evaluate, collect results
results = []
histories = {}
for name, build_fn in model_dict.items():
    print(f"\nTraining {name}...")
    model = build_fn((SEQ_LEN, len(features)))
    early_stop = EarlyStopping(patience=10, restore_best_weights=True)
    history = model.fit(X_train, y_train, validation_split=0.1, epochs=50, batch_size=16,
                        class_weight=class_weights_dict, callbacks=[early_stop], verbose=0)
    histories[name] = history
    y_pred_prob = model.predict(X_test).flatten()
    y_pred = (y_pred_prob > 0.5).astype(int)
    print(f"--- {name} Evaluation ---")
    print(classification_report(y_test, y_pred, digits=3))
    print(confusion_matrix(y_test, y_pred))

# Plot Loss Curves
'''plt.figure(figsize=(10, 6))
for name, history in histories.items():
    plt.plot(history.history['val_loss'], label=name)
plt.title("Validation Loss by Model")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
'''

"""Bidirectional LSTM good in :

- Recall (better at detecting actual bluebottle presence)

- F1-score (balanced precision/recall for minority class)    


Vanilla LSTM has high precision for class 0 but fails on class 1 (presence).

Class Imbalance not good:

- Only 46 out of 665 test examples are positive (presence = 1)

- Most models are biased toward predicting 0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Input
from tensorflow.keras.callbacks import EarlyStopping

def run_lstm_experiment(feature_list, seq_len=14, verbose=True):
    print(f"\n Running experiment with features:\n{feature_list}\n")

    # Load and preprocess
    df = pd.read_csv("final_data.csv")
    df['time'] = pd.to_datetime(df['time'])
    df = df.sort_values('time')
    df = df.drop(columns=['wave_sin_dir', 'wave_cos_dir'])

    df['month'] = df['time'].dt.month
    df['dayofweek'] = df['time'].dt.dayofweek
    target = 'presence'

    # Normalize
    scaler = MinMaxScaler()
    df_scaled = df.copy()
    df_scaled[feature_list] = scaler.fit_transform(df[feature_list])

    def create_sequences(data, labels, seq_len):
        X, y = [], []
        for i in range(len(data) - seq_len):
            X.append(data[i:i+seq_len])
            y.append(labels[i+seq_len])
        return np.array(X), np.array(y)

    X_all, y_all = create_sequences(df_scaled[feature_list].values, df[target].values, seq_len)
    split = int(len(X_all) * 0.8)
    X_train, X_test = X_all[:split], X_all[split:]
    y_train, y_test = y_all[:split], y_all[split:]

    # Class weights
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weights_dict = dict(enumerate(class_weights))

    # Build model
    model = Sequential([
        Input(shape=(seq_len, len(feature_list))),
        Bidirectional(LSTM(50, activation='tanh')),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    early_stop = EarlyStopping(patience=10, restore_best_weights=True)

    history = model.fit(X_train, y_train, validation_split=0.1, epochs=50, batch_size=16,
                        class_weight=class_weights_dict, callbacks=[early_stop], verbose=0)

    # Predict probabilities
    y_pred_prob = model.predict(X_test).flatten()

    # Try different thresholds
    for threshold in [0.5, 0.6, 0.7, 0.8]:
        y_pred = (y_pred_prob > threshold).astype(int)
        print(f"\n Threshold = {threshold}")
        print(classification_report(y_test, y_pred, digits=3))
        print("Confusion Matrix:")
        print(confusion_matrix(y_test, y_pred))

    # Plot loss curves
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title("Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Plot ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.title("ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Precision-Recall curve
    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)
    plt.figure(figsize=(6, 5))
    plt.plot(recall, precision)
    plt.title("Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.grid(True)
    plt.show()


# Try different feature sets by modifying this list

trial_features = [
    'crt_u',           # current
    'wave_hs',         # wave
    'wnd_dir',         # wind
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(trial_features)

trial_features = [
    'crt_temp',
    'crt_salt',           # current
    'wave_dir',         # wave
    'wnd_dir',         # wind
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(trial_features)

trial_features = [
    'crt_temp',
    'crt_salt',           # current
    'wave_dir',         # wave
    'wave_hs',
    'wnd_dir',         # wind
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(trial_features)

trial_features = [
    'crt_temp',
    'wave_dir',         # wave
    'wnd_dir',         # wind
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(trial_features)

trial_features = [
    'crt_temp',
    'wave_dir',         # wave
    'wnd_dir',         # wind
    'wave_t01',
    'wave_fp',
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(trial_features)

trial_features = [
    'crt_temp',
    'wave_dir',         # wave
    'wnd_dir',         # wind
    'wave_hs',
    'wave_t01',
    'wave_fp',
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(trial_features)

trial_features =['crt_temp',
                 'crt_u', 'crt_v',
                 'wave_dir', 'wave_hs', 'wave_t01', 'wave_fp',
                 'wnd_dir',
                 'month', 'dayofweek']

# Run the experiment
run_lstm_experiment(trial_features)

trial_features =['crt_temp',
                 'wave_dir', 'wave_hs', 'wave_t01', 'wave_fp',
                 'wnd_dir', 'month', 'dayofweek']

# Run the experiment
run_lstm_experiment(trial_features)

"""Best one:

['crt_temp',

'wave_dir', 'wave_hs', 'wave_t01', 'wave_fp',

'wnd_dir', 'month', 'dayofweek']

- At Threshold = 0.6:

- Recall (1) = 37.0%

- Precision (1) = 6.9%

- F1 (1) = 0.116

- TP = 17, FN = 29, FP = 230

# beach survey --- train; randwick--- test
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Input
from tensorflow.keras.callbacks import EarlyStopping

# Load datasets
df_train = pd.read_csv("merged_beach_surveys.csv")
df_test = pd.read_csv("merged_randwick.csv")

df_train['time'] = pd.to_datetime(df_train['time'])
df_test['time'] = pd.to_datetime(df_test['time'])

# Create time features
df_train['month'] = df_train['time'].dt.month
df_train['dayofweek'] = df_train['time'].dt.dayofweek
df_test['month'] = df_test['time'].dt.month
df_test['dayofweek'] = df_test['time'].dt.dayofweek

# Create 'presence' in training set from 'abundance'
df_train['presence'] = df_train['abundance'].fillna(0).apply(lambda x: 1 if x > 0 else 0)

# Compute 'wnd_dir' if missing
for df in [df_train, df_test]:
    if 'wnd_dir' not in df.columns:
        df['wnd_dir'] = (np.degrees(np.arctan2(df['wnd_uas'], df['wnd_vas'])) + 360) % 360

# Final function
def run_lstm_experiment(df_train, df_test, feature_list, seq_len=14):
    print(f"\n Running experiment with features:\n{feature_list}\n")

    # Normalize
    scaler = MinMaxScaler()
    df_train_scaled = df_train.copy()
    df_test_scaled = df_test.copy()
    df_train_scaled[feature_list] = scaler.fit_transform(df_train[feature_list])
    df_test_scaled[feature_list] = scaler.transform(df_test[feature_list])

    def create_sequences(data, labels, seq_len):
        X, y = [], []
        for i in range(len(data) - seq_len):
            X.append(data[i:i+seq_len])
            y.append(labels[i+seq_len])
        return np.array(X), np.array(y)

    X_train, y_train = create_sequences(df_train_scaled[feature_list].values, df_train_scaled['presence'].values, seq_len)
    X_test, y_test = create_sequences(df_test_scaled[feature_list].values, df_test_scaled['presence'].values, seq_len)

    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weights_dict = dict(enumerate(class_weights))

    model = Sequential([
        Input(shape=(seq_len, len(feature_list))),
        Bidirectional(LSTM(50, activation='tanh')),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    early_stop = EarlyStopping(patience=10, restore_best_weights=True)

    history = model.fit(X_train, y_train, validation_split=0.1, epochs=50, batch_size=16,
                        class_weight=class_weights_dict, callbacks=[early_stop], verbose=0)

    y_pred_prob = model.predict(X_test).flatten()

    for threshold in [0.5, 0.6, 0.7, 0.8]:
        y_pred = (y_pred_prob > threshold).astype(int)
        print(f"\n Threshold = {threshold}")
        print(classification_report(y_test, y_pred, digits=3))
        print("Confusion Matrix:")
        print(confusion_matrix(y_test, y_pred))

    # Loss plot
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title("Training & Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.show()

    # ROC
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.title("ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Precision-Recall
    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)
    plt.figure(figsize=(6, 5))
    plt.plot(recall, precision)
    plt.title("Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.grid(True)
    plt.show()

# Run
trial_features = ['crt_u', 'wave_hs', 'wnd_dir', 'month', 'dayofweek']
run_lstm_experiment(df_train, df_test, trial_features)

trial_features2 = [
    'crt_temp',
    'crt_salt',           # current
    'wave_dir',         # wave
    'wnd_dir',         # wind
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(df_train, df_test, trial_features2)




trial_features3 = [
    'crt_temp',
    'crt_salt',           # current
    'wave_dir',         # wave
    'wave_hs',
    'wnd_dir',         # wind
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(df_train, df_test, trial_features3)



trial_features4 = [
    'crt_temp',
    'wave_dir',         # wave
    'wnd_dir',         # wind
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(df_train, df_test, trial_features4)




trial_features5 = [
    'crt_temp',
    'wave_dir',         # wave
    'wnd_dir',         # wind
    'wave_t01',
    'wave_fp',
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(df_train, df_test, trial_features5)


trial_features6 = [
    'crt_temp',
    'wave_dir',         # wave
    'wnd_dir',         # wind
    'wave_hs',
    'wave_t01',
    'wave_fp',
    'month', 'dayofweek'  # temporal
]

# Run the experiment
run_lstm_experiment(df_train, df_test, trial_features6)




trial_features7 =['crt_temp',
                 'crt_u', 'crt_v',
                 'wave_dir', 'wave_hs', 'wave_t01', 'wave_fp',
                 'wnd_dir',
                 'month', 'dayofweek']

# Run the experiment
run_lstm_experiment(df_train, df_test, trial_features7)


trial_features8 =['crt_temp',
                 'wave_dir', 'wave_hs', 'wave_t01', 'wave_fp',
                 'wnd_dir', 'month', 'dayofweek']

# Run the experiment
run_lstm_experiment(df_train, df_test, trial_features8)

"""Best combination:
['crt_temp', 'wave_dir', 'wave_hs', 'wave_t01', 'wave_fp', 'wnd_dir', 'month', 'dayofweek']

- At Threshold = 0.8:

- Accuracy: 76.8%

- Recall (presence): 28.2%

- Precision (presence): 8.8%

- F1 (presence): 0.134

- True Positives: 69 / 245 presence events
"""

